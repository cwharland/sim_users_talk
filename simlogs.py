#!/usr/bin/env python
# encoding: utf-8

"""
Author: Chris Harland
"""

from __future__ import division
import pandas as pd
import numpy as np
from scipy import stats
import hashlib
import datetime

def create_hash_id(user, salt):
    """ returns a sha1 hash of user string combined with salt string """
    return hashlib.sha1(salt + '_' + repr(user)).hexdigest()

def get_beta_result(a,b, n = 1):
    """ takes a draw from beta(a,b) used to simulate random rates """
    return stats.beta.rvs(a,b, size = n)

def get_expon_result(mu, _lambda, n = 1):
    """ takes a draw from a exponential(mu, lambda) """
    return stats.expon.rvs(mu, _lambda, size = n)

def get_bernoulli_trial(p, n = 1):
    """ return a bernoulli trial of success or failure with probability p """
    return stats.bernoulli.rvs(p = p, size = n)

def get_exp_result(n,p, size = 1):
    """ return the outcome of n bernoulli trials with probability p """
    return stats.binom.rvs(n = n, p = p, size = size)

def gen_impression(p_imp = 1.0, p_click = 0.5, p_convert = 0.5):
    """ This function generates an impression, click, conversion based
    on probabilities defined by the input parameters """
    
    impression = get_bernoulli_trial(p_imp)[0]
    
    # Note: to speed this up would could draw all trials at once
    # and post process the results to make the outcomes conditional

    if impression == 1:
        did_click = get_bernoulli_trial(p_click)[0]
        # For now we assume only those that click can convert
        if did_click == 1:
            did_convert = get_bernoulli_trial(p_convert)[0]
        else:
            # Optionally this could be a bernoulli with a different p (i.e. the base rate)
            did_convert = 0
        imp_arr = [impression, did_click, did_convert]
        return imp_arr
    else:
        return None
    
def gen_log_line(uid, t_current = datetime.datetime.now(), p_imp = 1.0, p_click = 0.5, p_convert = 0.5):
    """ Get a log line for the given user and return with timestamp
    and impression info """
    
    imp = gen_impression(p_imp, p_click, p_convert)
    
    if imp is None:
        #print 'was none'
        return None
    else:
        # add a random t_delta
        delta_sec = stats.norm.rvs(loc = 300, scale = 100)
        t_ = t_current + datetime.timedelta(0,delta_sec)
        timestamp = t_.strftime('%Y-%m-%d %I:%M:%S%p')
        log_line = [timestamp, uid] + imp
        return log_line, t_
    
def gen_user_ids(n_users = 100, salt = 'fake_traffic'):
    """ returns a sha1 hash user id for n_users with the given salt """
    
    start_id = np.random.randint(0,2**8)
    
    return [create_hash_id(i, salt) for i in range(start_id, start_id + n_users)]

def pick_user(ids, weights, n = 1):
    """ picks a random user from ids with weight set as weights """
    
    # set uniform weights if none given
    if weights is None:
        weights = np.ones(len(ids))
    
    # check weight norm
    if sum(weights) != 1:
        weights = weights/sum(weights)
    
    return np.random.choice(ids, size = n, p = weights)

def gen_user_visit_freq(n_users = 100, _lambda = 2):
    """ return the total number of visits in a set time delta for the number of given users """
    return stats.poisson.rvs(mu = _lambda, size = n_users)

def gen_beta_params(p, n = 1000):
    """ generates alpha and beta for beta distribution from a given target conversion rate and strength n """
    alpha = int(p*n)
    beta = n - alpha
    return alpha, beta

def return_pairing(source, inds, target):
    """ returns the elements of target corresponding to paired elements of source
    drawn in the sequence inds.  An example would be a source of user_ids that have
    assigned properties from target and the users were sampled in the sequence inds"""
    
    return [target[y] for y in [source.index(x) for x in inds]]

def log_to_df(log_arr):
    """ takes an array of logs lines generated by simulation and returns a dataframe """

    # hardcoding these for now but we can make it user defined later
    col_names = ['timestamp','user_id','impression','click','conversion']
    df = pd.DataFrame(log_arr, columns = col_names)
    df['timestamp'] = pd.to_datetime(df['timestamp'].values)
    df['impression'] = df.impression.astype(int)
    df['click'] = df.click.astype(int)
    df['conversion'] = df.conversion.astype(int)
    
    return df.sort('timestamp').reset_index(drop = True)

def simulate_log_vectorized(n_users = 100, n_rows = 1000, p_imp = 1.0, p_click = 0.5, p_convert = 0.5, strict = False):
    """ wrapper to generate logs, trying to refactor to vectorize all functions to speed up simulation 
        we need vectorize all the called functions to speed this up but this is a start """
    # get users
    uids = gen_user_ids(n_users)
    # get visit likelihood
    visit_freq = gen_user_visit_freq(n_users)
    # get probiblity of actions
    if strict == True:
        p_clicks = np.repeat(p_click,n_users)
        p_converts = np.repeat(p_convert, n_users)
    else:
        params = gen_beta_params(p_click)
        p_clicks = stats.beta.rvs(a = params[0], b = params[1], size = n_users)
        params = gen_beta_params(p_convert)
        p_converts = stats.beta.rvs(a = params[0], b = params[1], size = n_users)
    p_imp_order = np.repeat(p_imp,n_rows)
    
    # generate user sequence
    uid_order = pick_user(uids, visit_freq, n_rows)
    
    # The users have predetermined probabilities for click and conversion assigned to them
    # so we simply need to pull those probabilities in the order of the uids drawn
    p_click_order = return_pairing(uids, uid_order, p_clicks)
    p_convert_order = return_pairing(uids, uid_order, p_converts)
    
    # gen timestamps
    t_ = datetime.datetime.now() + np.cumsum([datetime.timedelta(0,x) for x in stats.norm.rvs(loc = 300, scale = 100, size = n_rows)])
    
    # This, plus the vector of draws from above are the only real speed ups here.  gen_log_line and its called functions are not
    log_arr = [gen_log_line(u,v,w,x,y)[0] for (u,v,w,x,y) in zip(uid_order, t_, p_imp_order, p_click_order, p_convert_order)]
    
    return log_to_df(log_arr)
    
    
def simulate_log(n_users = 100, n_rows = 1000, p_imp = 1.0, p_click = 0.5, p_convert = 0.5):
    """ wrapper function to generate a log for n_users over n_rows vists """
    
    # get users
    uids = gen_user_ids(n_users)
    # get visit likelihood
    visit_freq = gen_user_visit_freq(n_users)
    # get probiblity of actions
    params = gen_beta_params(p_click)
    p_clicks = stats.beta.rvs(a = params[0], b = params[1], size = n_users)
    params = gen_beta_params(p_convert)
    p_converts = stats.beta.rvs(a = params[0], b = params[1], size = n_users)
    
    # preallocate in the future
    log_arr = []
    t_ = datetime.datetime.now()
    
    i = 0
    while i < n_rows:
        uid = pick_user(uids, visit_freq)[0]
        p_click = p_clicks[uids == uid]
        p_convert = p_converts[uids == uid]
        line, t_ = gen_log_line(uid,t_, p_imp, p_click, p_convert)
        
        if line is None:
            continue
        else:
            log_arr = log_arr + line
            i += 1
    
    # fix output shape
    log_arr = np.reshape(log_arr, (n_rows,-1))
    
    return log_to_df(log_arr)

### stats functions
def fix_pvalue(pval):
    if pval > 0.5:
        return 2*(1-pval)
    else:
        return 2*(pval)

def quick_wald(mu_c,sd_c,n_c,mu_t,sd_t,n_t,conf = 0.95):

    z = abs(stats.norm.ppf((1-conf) / 2))
    
    se_c = sd_c / np.sqrt(n_c)
    
    se_t = sd_t / np.sqrt(n_t)
    
    # Find the diff, get the diff se, grab z-score for p-value
    mu_diff = mu_t - mu_c
    se_diff = np.sqrt(se_c**2 + se_t**2)
    z_score = mu_diff / se_diff
    
    # standard CI
    lift = mu_diff / mu_c
    ll = (mu_diff - z*se_diff) / mu_c
    ul = (mu_diff + z*se_diff) / mu_c
    
    # turns the one sided p-val from given zscore and gets you a two-sided
    pval = fix_pvalue(stats.norm.cdf(z_score))
    
    return lift,ll,ul,pval
    
def wald_test(control, treatment, conf = 0.95):
    """ this function takes two samples and computes the wald test for significance in mean difference with the normality assumption """
    
    z = abs(stats.norm.ppf((1-conf) / 2))
    
    # optionally we could bootstrap the CI on mu_c and mu_t and remove the normality constraint
    mu_c = np.mean(control)
    se_c = np.std(control) / np.sqrt(len(control))
    
    mu_t = np.mean(treatment)
    se_t = np.std(treatment) / np.sqrt(len(treatment))
    
    # Find the diff, get the diff se, grab z-score for p-value
    mu_diff = mu_t - mu_c
    se_diff = np.sqrt(se_c**2 + se_t**2)
    z_score = mu_diff / se_diff
    
    # standard CI
    lift = mu_diff / mu_c
    ll = (mu_diff - z*se_diff) / mu_c
    ul = (mu_diff + z*se_diff) / mu_c
    
    # turns the one sided p-val from given zscore and gets you a two-sided
    pval = fix_pvalue(stats.norm.cdf(z_score))
    
    return lift,ll,ul,pval

def wald_ci(s1,n1,s2,n2,conf = 0.95, ac_correct = True):
    """ wald ci for binomial proportions (similar to above) with optional agresti-coull corrections for better coverage """
    if ac_correct == True:
        n1 += 4
        n2 += 4
        p1hat = (s1 + 2) / n1
        p2hat = (s2 + 2) / n2
    else:
        p1hat = s1 / n1
        p2hat = s2 / n2
    
    z = abs(stats.norm.ppf((1 - conf)/2))
    p1se = np.sqrt(p1hat * (1 - p1hat) / n1)
    p2se = np.sqrt(p2hat * (1 - p2hat) / n2)
    
    p_diff = p2hat - p1hat
    se_diff = np.sqrt(p1se**2 + p2se**2)
    z_score = p_diff / se_diff
    
    lift = p_diff / p1hat
    ll = (p_diff - z * se_diff) / p1hat
    ul = (p_diff + z * se_diff) / p1hat
    
    pval = fix_pvalue(stats.norm.cdf(z_score))
    
    return lift,ll,ul,pval